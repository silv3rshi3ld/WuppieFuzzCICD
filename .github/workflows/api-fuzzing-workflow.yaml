# API Fuzzing Pipeline with Docker Swarm

name: API Fuzzing Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

permissions:
  contents: read

env:
  DOCKER_NETWORK_NAME: cicd_network
  VAMPI_RESTLER_PORT: 5012
  VAMPI_WUPPIEFUZZ_PORT: 5022
  VAMPI_EVOMASTER_PORT: 5032
  BASE_DIR: ${{ github.workspace }}
  FUZZERS: restler wuppiefuzz evomaster
  MAX_STARTUP_RETRIES: 3
  HEALTH_CHECK_TIMEOUT: 30
  NETWORK_TIMEOUT: 60

jobs:
  # 1. Initial cleanup
  cleanup:
    runs-on: self-hosted
    steps:
      - name: Cleanup Previous Setup
        run: |
          echo "Cleaning up previous setup..."
          
          # Kill any processes using known ports
          for port in ${{ env.VAMPI_RESTLER_PORT }} ${{ env.VAMPI_WUPPIEFUZZ_PORT }} ${{ env.VAMPI_EVOMASTER_PORT }}; do
            pid=$(lsof -ti :$port) || true
            if [ -n "$pid" ]; then
              echo "Killing process using port $port"
              kill -9 $pid || echo "Failed to kill process on port $port"
            fi
          done
          
          # Check if we're in swarm mode before attempting stack removal
          if sudo docker info 2>/dev/null | grep -q "Swarm: active"; then
            echo "Removing existing stack..."
            sudo docker stack rm fuzzing-stack || true
            # Wait for stack to be fully removed
            while sudo docker stack ls | grep -q "fuzzing-stack"; do
              echo "Waiting for stack removal..."
              sleep 5
            done
          else
            echo "Not in swarm mode, skipping stack removal"
          fi
          
          # Stop registry container if running, but preserve its data
          if sudo docker ps --filter "name=registry" --format '{{.Names}}' | grep -q "^registry$"; then
            echo "Stopping registry container..."
            sudo docker stop registry || true
            sudo docker rm registry || true
          fi
          
          # Cleanup running containers
          if sudo docker ps -q | grep -q .; then
            echo "Stopping running containers..."
            sudo docker ps -q | xargs -r sudo docker stop -t 0 || true
            echo "Removing containers..."
            sudo docker ps -aq | xargs -r sudo docker rm -f || true
          else
            echo "No running containers to clean up"
          fi
          
          # Remove vampi and fuzzer images if they exist
          if sudo docker images "localhost:5000/vampi-vulnerable-*" --format "{{.ID}}" | grep -q .; then
            echo "Removing vampi images..."
            sudo docker images "localhost:5000/vampi-vulnerable-*" --format "{{.ID}}" | xargs -r sudo docker rmi -f
          fi
          if sudo docker images "localhost:5000/*" --format "{{.ID}}" | grep -q .; then
            echo "Removing fuzzer images..."
            sudo docker images "localhost:5000/*" --format "{{.ID}}" | xargs -r sudo docker rmi -f
          fi
          
          # Cleanup Docker network if it exists
          if sudo docker network ls | grep -q "${{ env.DOCKER_NETWORK_NAME }}"; then
            echo "Removing Docker network..."
            sudo docker network rm "${{ env.DOCKER_NETWORK_NAME }}" || true
          fi
          
          # Prune unused resources
          echo "Pruning unused Docker resources..."
          sudo docker container prune -f
          sudo docker network prune -f
          
          echo "Cleanup completed."
          sleep 5
  # 2. Initialize Swarm infrastructure
  initialize_swarm:
    needs: cleanup
    runs-on: self-hosted
    outputs:
      swarm-manager-ip: ${{ steps.setup_swarm.outputs.swarm-manager-ip }}
      swarm-worker-token: ${{ steps.setup_swarm.outputs.swarm-worker-token }}
      network-id: ${{ steps.setup_network.outputs.network-id }}
      manager-hostname: ${{ steps.setup_swarm.outputs.manager-hostname }}
      swarm-initialized: ${{ steps.setup_swarm.outputs.swarm-initialized }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Count Active Runners
        id: count_runners
        run: |
          echo "TOTAL_RUNNERS=$(gh api /repos/${{ github.repository }}/actions/runners | jq '.total_count')" >> $GITHUB_ENV
        env:
          GH_TOKEN: ${{ github.token }}

      - name: Make manage-swarm.sh executable
        run: chmod +x ./scripts/manage-swarm.sh

      - name: Manage Swarm State
        id: setup_swarm
        env:
          MANAGER_IP: ${{ secrets.MANAGER_IP }}
        run: |
          # Run manage-swarm.sh and capture its exit status
          if ./scripts/manage-swarm.sh ${{ env.TOTAL_RUNNERS }}; then
            # Only set outputs if swarm initialization succeeded
            echo "swarm-manager-ip=$MANAGER_IP" >> $GITHUB_OUTPUT
            WORKER_TOKEN=$(sudo docker swarm join-token worker -q) || exit 1
            echo "swarm-worker-token=$WORKER_TOKEN" >> $GITHUB_OUTPUT
            echo "manager-hostname=$(hostname)" >> $GITHUB_OUTPUT
            echo "swarm-initialized=true" >> $GITHUB_OUTPUT
          else
            echo "::error::Swarm initialization failed"
            exit 1
          fi
      - name: Setup Docker Network
        id: setup_network
        run: |
          # Check if network exists first
          if ! sudo docker network ls | grep -q "${{ env.DOCKER_NETWORK_NAME }}"; then
            echo "Creating overlay network ${{ env.DOCKER_NETWORK_NAME }}"
            NETWORK_ID=$(sudo docker network create --driver overlay --attachable ${{ env.DOCKER_NETWORK_NAME }})
            echo "network-id=$NETWORK_ID" >> $GITHUB_OUTPUT
          else
            echo "Network ${{ env.DOCKER_NETWORK_NAME }} already exists"
            NETWORK_ID=$(sudo docker network ls --filter name=${{ env.DOCKER_NETWORK_NAME }} --format "{{.ID}}")
            echo "network-id=$NETWORK_ID" >> $GITHUB_OUTPUT
          fi
          
          # Verify network was created/exists and is overlay type
          if ! sudo docker network inspect ${{ env.DOCKER_NETWORK_NAME }} --format "{{.Driver}}" | grep -q "overlay"; then
            echo "::error::Network ${{ env.DOCKER_NETWORK_NAME }} is not an overlay network"
            exit 1
          fi
  # 3. Configure Swarm Workers
  configure_workers:
    needs: initialize_swarm
    runs-on: self-hosted
    steps:
      - name: Debug Runner Info
        run: |
          echo "Runner Name: ${{ runner.name }}"
          echo "Hostname: $(hostname)"
          echo "Manager Hostname: ${{ needs.initialize_swarm.outputs.manager-hostname }}"
      - name: Join Swarm (Worker Node)
        if: needs.initialize_swarm.outputs.swarm-initialized == 'true' && runner.name != needs.initialize_swarm.outputs.manager-hostname
        env:
          MANAGER_IP: ${{ secrets.MANAGER_IP }}
        run: |
          # Check if already in swarm
          if sudo docker info 2>/dev/null | grep -q "Swarm: active"; then
            echo "Node is already part of a swarm"
            exit 0
          fi
          # Configure Docker daemon
          sudo systemctl stop docker || true
          sudo sed -i 's/^ExecStart=.*/ExecStart=\/usr\/bin\/dockerd -H fd:\/\/ --containerd=\/run\/containerd\/containerd.sock --iptables=true --ip-masq=true --ip-forward=true/' /lib/systemd/system/docker.service
          sudo systemctl daemon-reload
          sudo systemctl start docker
          
          # Wait for Docker to be ready
          timeout 30s bash -c 'until sudo docker info &>/dev/null; do sleep 1; done'
          
          # Verify manager IP
          echo "Attempting to join swarm at $MANAGER_IP:2377"
          if ! ping -c 1 $MANAGER_IP > /dev/null 2>&1; then
            echo "::error::Cannot ping manager at $MANAGER_IP"
            exit 1
          fi
          
          # Join swarm
          if ! timeout 30s sudo docker swarm join --token ${{ needs.initialize_swarm.outputs.swarm-worker-token }} $MANAGER_IP:2377; then
            echo "::error::Failed to join swarm"
            exit 1
          fi
      - name: Label Workers (Manager Only)
        if: needs.initialize_swarm.outputs.swarm-initialized == 'true' && runner.name == needs.initialize_swarm.outputs.manager-hostname
        run: |
          # Get all worker nodes
          WORKER_NODES=($(sudo docker node ls --format '{{.Hostname}}' --filter role=worker))
          FUZZERS=(restler wuppiefuzz evomaster)
          
          # Label each worker with a different fuzzer
          for i in "${!WORKER_NODES[@]}"; do
            if [ $i -lt ${#FUZZERS[@]} ]; then
              WORKER="${WORKER_NODES[$i]}"
              FUZZER="${FUZZERS[$i]}"
              echo "Labeling worker node $WORKER for fuzzer $FUZZER"
              sudo docker node update --label-add type=fuzzer --label-add fuzzer=$FUZZER "$WORKER"
            fi
          done
  # 4. Verify Swarm setup
  verify_swarm:
    needs: [initialize_swarm, configure_workers]
    runs-on: self-hosted
    if: success()
    steps:
      - name: Verify Swarm and Network
        run: |
          # Verify swarm mode is active
          if ! sudo docker info | grep -q "Swarm: active"; then
            echo "::error::Not in swarm mode - swarm initialization may have failed"
            exit 1
          fi
          
          # Verify network exists and is overlay type
          if ! sudo docker network ls --filter driver=overlay | grep -q "${{ env.DOCKER_NETWORK_NAME }}"; then
            echo "::error::Required overlay network ${{ env.DOCKER_NETWORK_NAME }} not found"
            sudo docker network ls
            exit 1
          fi
          
          # List nodes and their roles
          echo "Swarm Nodes:"
          sudo docker node ls
          
          echo "Swarm configuration verified successfully"
  # 5. Prepare OpenAPI specs
  prepare_specs:
    needs: verify_swarm
    runs-on: self-hosted
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4.0.0

      - name: Copy OpenAPI Specs to Volumes
        run: |
          # Create temporary directory
          mkdir -p specs_temp
          cp services/vampi/openapi_specs/openapi3.yml specs_temp/
          
          # Copy specs to volumes
          sudo docker run --rm \
            -v $(pwd)/specs_temp:/source \
            -v openapi_spec:/dest \
            alpine cp /source/openapi3.yml /dest/
  # 6. Build and push images
  build_and_push_images:
    needs: verify_swarm
    runs-on: self-hosted
    strategy:
      matrix:
        service: [vampi-vulnerable, wuppiefuzz, restler, evomaster]
      fail-fast: false
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4.0.0

      - name: Setup Local Registry
        run: |
          # Check if registry exists and is running
          if ! sudo docker ps --filter "name=registry" --format '{{.Names}}' | grep -q "^registry$"; then
            echo "Starting new registry container..."
            sudo docker run -d \
              --name registry \
              --restart=always \
              -v registry_data:/var/lib/registry \
              -p 5000:5000 \
              registry:2
          else
            echo "Registry container already running"
          fi
          
          # Wait for registry to be ready
          for i in $(seq 1 30); do
            if curl -s http://localhost:5000/v2/ > /dev/null; then
              echo "Registry is ready"
              break
            fi
            if [ $i -eq 30 ]; then
              echo "Registry failed to start"
              exit 1
            fi
            echo "Waiting for registry... ($i/30)"
            sleep 1
          done
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          driver-opts: |
            network=host
          buildkitd-flags: --debug

      - name: Set up Build Cache
        uses: actions/cache@v3
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ matrix.service }}-${{ steps.hash.outputs.content_hash }}
          restore-keys: |
            ${{ runner.os }}-buildx-${{ matrix.service }}-
            ${{ runner.os }}-buildx-
      - name: Calculate Image Hash
        id: hash
        run: |
          # Set registry address
          REGISTRY=localhost:5000
          
          # Determine service directory
          SERVICE_DIR=services/${{ matrix.service == 'vampi-vulnerable' && 'vampi' || matrix.service }}
          
          # Calculate hash of all files that affect the build
          if [ "${{ matrix.service }}" = "wuppiefuzz" ]; then
            # For WuppieFuzz, include the remote repo in hash calculation
            REMOTE_HASH=$(git ls-remote https://github.com/TNO-S3/WuppieFuzz.git HEAD | cut -f1)
            echo "WuppieFuzz remote hash: $REMOTE_HASH"
            HASH=$(echo "$REMOTE_HASH" | sha256sum | cut -d' ' -f1)
          else
            # For other services, hash the local files
            HASH=$(find $SERVICE_DIR -type f -exec sha256sum {} \; | sort | sha256sum | cut -d' ' -f1)
          fi
          
          echo "CONTENT_HASH=$HASH" >> $GITHUB_ENV
          echo "content_hash=$HASH" >> $GITHUB_OUTPUT

      - name: Check Existing Image
        id: check_image
        run: |
          REGISTRY=localhost:5000
          SERVICE=${{ matrix.service }}
          HASH=${{ steps.hash.outputs.content_hash }}
          
          if [ "$SERVICE" = "vampi-vulnerable" ]; then
            for FUZZER in restler wuppiefuzz evomaster; do
              # Check if image with this hash exists
              if curl -s "http://localhost:5000/v2/vampi-vulnerable-$FUZZER/tags/list" | grep -q "\"$HASH\""; then
                echo "Image vampi-vulnerable-$FUZZER:$HASH already exists"
                echo "VAMPI_${FUZZER}_TAG=$HASH" >> $GITHUB_ENV
              else
                echo "Need to build vampi-vulnerable-$FUZZER:$HASH"
                echo "BUILD_VAMPI_${FUZZER}=true" >> $GITHUB_ENV
              fi
            done
          else
            # Check if image with this hash exists
            if curl -s "http://localhost:5000/v2/$SERVICE/tags/list" | grep -q "\"$HASH\""; then
              echo "Image $SERVICE:$HASH already exists"
              echo "BUILD_NEEDED=false" >> $GITHUB_ENV
            else
              echo "Need to build $SERVICE:$HASH"
              echo "BUILD_NEEDED=true" >> $GITHUB_ENV
            fi
          fi

      - name: Build and Push Image
        run: |
          REGISTRY=localhost:5000
          SERVICE_DIR=services/${{ matrix.service == 'vampi-vulnerable' && 'vampi' || matrix.service }}
          DOCKERFILE=${{ matrix.service != 'vampi-vulnerable' && format('Dockerfile.{0}', matrix.service) || 'Dockerfile' }}
          HASH=${{ steps.hash.outputs.content_hash }}
          
          # For WuppieFuzz, ensure source code is available if needed
          if [ "${{ matrix.service }}" = "wuppiefuzz" ] && [ "$BUILD_NEEDED" = "true" ]; then
            echo "Preparing WuppieFuzz build context..."
            git clone https://github.com/TNO-S3/WuppieFuzz.git temp_wuppiefuzz
            cp -r temp_wuppiefuzz/* $SERVICE_DIR/
            rm -rf temp_wuppiefuzz
          fi
          
          cd $SERVICE_DIR
          
          # Special handling for VAmPI instances
          if [ "${{ matrix.service }}" = "vampi-vulnerable" ]; then
            for FUZZER in restler wuppiefuzz evomaster; do
              BUILD_VAR="BUILD_VAMPI_${FUZZER}"
              if [ "${!BUILD_VAR}" = "true" ]; then
                echo "Building VAmPI for $FUZZER"
                sudo docker buildx build \
                  --platform linux/amd64 \
                  --file $DOCKERFILE \
                  --build-arg vulnerable=1 \
                  --build-arg BUILDKIT_INLINE_CACHE=1 \
                  --cache-from type=local,src=/tmp/.buildx-cache \
                  --cache-to type=local,dest=/tmp/.buildx-cache-new,mode=max \
                  --tag $REGISTRY/vampi-vulnerable-$FUZZER:$HASH \
                  --network host \
                  --push \
                  .
                
                echo "VAMPI_${FUZZER}_TAG=$HASH" >> $GITHUB_ENV
              fi
            done
          elif [ "$BUILD_NEEDED" = "true" ]; then
            # Build other services only if needed
            sudo docker buildx build \
              --platform linux/amd64 \
              --file $DOCKERFILE \
              --build-arg BUILDKIT_INLINE_CACHE=1 \
              --cache-from type=local,src=/tmp/.buildx-cache \
              --cache-to type=local,dest=/tmp/.buildx-cache-new,mode=max \
              --tag $REGISTRY/${{ matrix.service }}:$HASH \
              --tag $REGISTRY/${{ matrix.service }}:latest \
              --network host \
              --push \
              .
          fi

          # Move cache
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache
  # 7. Deploy stack
  deploy_stack:
    needs: [prepare_specs, build_and_push_images]
    runs-on: self-hosted
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4.0.0

      - name: Deploy Stack
        run: |
          # Make deploy script executable
          sudo chmod +x ./scripts/deploy-stack.sh
          
          # Run deploy script
          sudo ./scripts/deploy-stack.sh
  # 8. Run fuzzing
  run_fuzzing:
    needs: deploy_stack
    runs-on: self-hosted
    strategy:
      matrix:
        fuzzer: [restler, wuppiefuzz, evomaster]
      fail-fast: false
    steps:
      - name: Run Fuzzing Tests
        run: |
          # Determine VAmPI port based on fuzzer
          case "${{ matrix.fuzzer }}" in
            "restler")
              VAMPI_PORT=${{ env.VAMPI_RESTLER_PORT }}
              ;;
            "wuppiefuzz")
              VAMPI_PORT=${{ env.VAMPI_WUPPIEFUZZ_PORT }}
              ;;
            "evomaster")
              VAMPI_PORT=${{ env.VAMPI_EVOMASTER_PORT }}
              ;;
          esac
          
          # Wait for VAmPI to be ready
          echo "Waiting for VAmPI (${{ matrix.fuzzer }}) to be ready..."
          for i in $(seq 1 30); do
            if curl -s http://localhost:$VAMPI_PORT/health > /dev/null; then
              echo "VAmPI is ready"
              break
            fi
            if [ $i -eq 30 ]; then
              echo "::error::VAmPI failed to become ready"
              exit 1
            fi
            echo "Waiting for VAmPI... ($i/30)"
            sleep 2
          done
          # Monitor fuzzing services
          echo "Monitoring fuzzing services..."
          TIMEOUT=3600  # 1 hour timeout
          START_TIME=$(date +%s)
          
          while true; do
            CURRENT_TIME=$(date +%s)
            ELAPSED_TIME=$((CURRENT_TIME - START_TIME))
            
            if [ $ELAPSED_TIME -gt $TIMEOUT ]; then
              echo "Fuzzing timeout reached"
              break
            fi
            
            # Check service status
            if ! sudo docker stack services fuzzing-stack --format "{{.Name}} {{.Replicas}}"; then
              echo "::error::Failed to get service status"
              exit 1
            fi
            
            # Check for completed fuzzing
            COMPLETED_COUNT=$(sudo docker service logs fuzzing-stack_${{ matrix.fuzzer }} 2>&1 | grep -c "Fuzzing completed" || true)
            
            if [ $COMPLETED_COUNT -gt 0 ]; then
              echo "Fuzzing completed successfully"
              break
            fi
            
            echo "Fuzzing in progress... (${ELAPSED_TIME}s elapsed)"
            sleep 30
          done
      - name: Collect Results
        run: |
          mkdir -p fuzzing_results/${{ matrix.fuzzer }}
          
          # Copy results from volumes
          sudo docker run --rm \
            -v ${{ matrix.fuzzer }}_output:/results \
            -v $(pwd)/fuzzing_results/${{ matrix.fuzzer }}:/output \
            alpine cp -r /results/* /output/
      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: fuzzing-results-${{ matrix.fuzzer }}
          path: fuzzing_results/${{ matrix.fuzzer }}
          retention-days: 7

  # 9. Summarize results
  summarize_results:
    needs: run_fuzzing
    runs-on: self-hosted
    if: always()
    steps:
      - name: Download Results
        uses: actions/download-artifact@v4
        with:
          pattern: fuzzing-results-*
          path: all_results
          merge-multiple: true

      - name: List Results
        run: |
          echo "Listing Results:"
          ls -la all_results || echo "No results found."
      - name: Final Cleanup
        run: |
          echo "No additional cleanup needed - already handled in initial cleanup job"
