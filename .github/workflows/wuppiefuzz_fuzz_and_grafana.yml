name: WuppieFuzz Build and Fuzz

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  build_and_fuzz:
    runs-on: self-hosted

    steps:
      # Step 1: Checkout the repository
      - name: Checkout repository
        uses: actions/checkout@v4

      # Step 2: Get WuppieFuzz commit hash
      - name: Get WuppieFuzz commit hash
        id: wuppiefuzz_commit
        run: |
          git ls-remote https://github.com/TNO-S3/WuppieFuzz.git HEAD | awk '{print $1}' > wuppiefuzz_commit.txt
          echo "commit_hash=$(cat wuppiefuzz_commit.txt)" >> $GITHUB_ENV

      # Step 3: Cache Rust dependencies and build
      - name: Cache Rust dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ env.commit_hash }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      # Step 4: Install Docker Compose
      - name: Install Docker Compose
        run: |
          # Update package lists and install Docker Compose
          sudo apt-get update
          sudo apt-get install -y docker-compose
          docker-compose --version  # Verify the installation

      # Step 5: Set up Docker Buildx
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3 # Use Docker Buildx for advanced multi-platform builds

      # Step 6: Build and run services with Docker Compose
      - name: Build and run services with Docker Compose
        env:
          # Use secrets stored in GitHub to provide secure access to SMTP credentials
          SMTP_USER: ${{ secrets.SMTP_USER }}
          SMTP_PASS: ${{ secrets.SMTP_PASS }}
          SMTP_HOST: ${{ secrets.SMTP_HOST }}
          SMTP_PORT: ${{ secrets.SMTP_PORT }}
        run: |
          # Create a shared Docker network
          docker network create wuppycicd_network || true

          # Build and start the Docker services defined in the Docker Compose file
          docker-compose up -d --build

      # Step 7: Wait for services to start
      - name: Wait for services to start
        run: sleep 15  # Wait for 15 seconds to allow services to initialize

      # Step 8: Test API Connectivity
      - name: Test API Connectivity
        run: |
          # Test if the API service is running by sending a HEAD request
          curl -I http://localhost:3001

      # Step 9: Install Rust toolchain
      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      # Step 10: Install dependencies for WuppieFuzz
      - name: Install dependencies for WuppieFuzz
        run: |
          sudo apt-get update
          sudo apt-get install -y pkg-config libssl-dev

      # Step 11: Clone WuppieFuzz repository
      - name: Clone WuppieFuzz repository
        run: git clone https://github.com/TNO-S3/WuppieFuzz.git

      # Step 12: Build WuppieFuzz with cached dependencies
      - name: Build WuppieFuzz
        run: |
          cd WuppieFuzz
          cargo build --release
          cd ..

      # Step 13: Add WuppieFuzz to PATH
      - name: Add WuppieFuzz to PATH
        run: |
          # Add the WuppieFuzz binary to the system PATH for easy execution
          echo "${{ github.workspace }}/WuppieFuzz/target/release" >> $GITHUB_PATH

      # Step 14: Generate initial corpus
      - name: Generate initial corpus
        run: wuppiefuzz output-corpus --openapi-spec openapi.yaml corpus_directory

      # Step 15: Run WuppieFuzz fuzzing
      - name: Run WuppieFuzz
        env:
          RUST_BACKTRACE: 1
        run: |
          wuppiefuzz fuzz --report --log-level info --initial-corpus corpus_directory \
          --timeout 60 openapi.yaml

      # Step 16: Upload WuppieFuzz report
      - name: Upload WuppieFuzz report
        uses: actions/upload-artifact@v4
        with:
          name: wuppiefuzz-report
          path: reports/

      # Step 17: Upload report.db artifact
      - name: Upload report.db
        uses: actions/upload-artifact@v4
        with:
          name: report-db
          path: reports/report.db

      # Step 18: Stop and remove Docker containers
      - name: Stop and remove Docker containers
        if: always()
        run: docker-compose down

  grafana_setup:
    runs-on: self-hosted
    needs: build_and_fuzz
    steps:
      # Step 1: Set up Grafana with SQLite plugin using Docker Compose
      - name: Set up Grafana
        run: |
          # Create a shared Docker network
          docker network create wuppycicd_network || true

          # Create a Docker Compose file to set up Grafana with the SQLite data source plugin
          cat <<EOF > docker-compose-grafana.yml
          version: '3'
          services:
            grafana:
              image: grafana/grafana:latest
              ports:
                - "3000:3000" # Expose Grafana on port 3000
              networks:
                - wuppycicd_network
              volumes:
                - ./report.db:/var/lib/grafana/data/report.db # Mount the report.db file for Grafana to access
                - ./grafana.ini:/etc/grafana/grafana.ini # Use a custom Grafana configuration
              environment:
                - GF_SECURITY_ADMIN_PASSWORD=${{ secrets.GRAFANA_ADMIN_PASSWORD }} # Set Grafana admin password securely
                - GF_INSTALL_PLUGINS=grafana-sqlite-datasource # Install the SQLite plugin for Grafana
          networks:
            wuppycicd_network:
              external: true
          EOF

          # Start Grafana using Docker Compose
          docker-compose -f docker-compose-grafana.yml up -d

      # Step 2: Wait for Grafana to start
      - name: Wait for Grafana to start
        run: sleep 15 # Wait for 15 seconds to ensure Grafana is ready

      # Step 3: Provision Grafana dashboards
      - name: Provision Grafana dashboards
        run: |
          # Copy the pre-configured dashboards and data source settings into the Grafana container
          docker cp dashboards.yaml grafana:/etc/grafana/provisioning/dashboards/
          docker cp datasources.yaml grafana:/etc/grafana/provisioning/datasources/
